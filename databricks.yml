# This is a Databricks asset bundle definition for vs_code_extension.
# The Databricks extension requires databricks.yml configuration file.
# See https://docs.databricks.com/dev-tools/bundles/index.html for documentation.

# Bundle validation, deployment, and run commands
# databricks bundle validate --profile DEFAULT
# databricks bundle deploy -t spark_python_task --profile DEFAULT
# databricks bundle run train --profile DEFAULT
bundle:
  name: vscode_extension

workspace:
  host: https://e2-demo-field-eng.cloud.databricks.com/
  profile: DEFAULT

variables:
  experiment_name:
    description: Experiment name for model training
    default: '/Users/${workspace.current_user.userName}/vscode_extension_experiment'
  delta_location:
    description: Feature table Delta location
    default: 'main.default.titanic_features'
  dbfs_csv_folder:
    description: Folder that houses the raw data csv file in Databricks
    default: dbfs:/Shared/temp_data
   
# Resources that will be deployed in each target
resources:
  experiments:
    experiment:
      name: ${var.experiment_name}.${bundle.target}
  jobs: 
    train:
        name: training
        job_clusters:
          - job_cluster_key: job_cluster
            new_cluster:
              spark_version: 15.4.x-cpu-ml-scala2.12
              node_type_id: i3.xlarge
              data_security_mode: SINGLE_USER
              runtime_engine: STANDARD
              num_workers: 0
              spark_conf: {
                "spark.databricks.cluster.profile": "singleNode",
                "spark.master": "local[*]"
                }
              custom_tags: {
                "ResourceClass": "SingleNode"
                }

targets:
  spark_python_task:
    # Extended the job configuration referenced above
    resources:
      jobs: 
        train:
            name: training
            git_source:
                git_branch: main
                git_provider: gitHub
                git_url: https://github.com/marshackVB/vscode_extension.git
            tasks:
              - task_key: spark_python_task
                job_cluster_key: job_cluster
                spark_python_task:
                  python_file: main/train.py
                  source: GIT
                  parameters: ["--experiment_name", "${var.experiment_name}",
                               "--delta_location", "${var.delta_location}",
                               "--dbfs_csv_folder", "${var.dbfs_csv_folder}"] 

  python_wheel_task:
    # https://docs.databricks.com/en/dev-tools/bundles/python-wheel.html
    resources:
      jobs: 
        train:
            name: training
            tasks:
              - task_key: python_wheel_task
                # To run the job on an existing, interactive cluster
                existing_cluster_id: "0920-204155-vpwxm182"
                #job_cluster_key: job_cluster
                python_wheel_task:
                  package_name: vscode_extension
                  entry_point: train
                  named_parameters: {"experiment_name": "${var.experiment_name}",
                                     "delta_location": "${var.delta_location}",
                                     "dbfs_csv_folder": "${var.dbfs_csv_folder}"}
                libraries:
              # By default we just include the .whl file generated for the package_deployment package.
              # See https://docs.databricks.com/dev-tools/bundles/library-dependencies.html
              # for more information on how to add other libraries.
                - whl: dist/*.whl